{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceff32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291eeee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pid                                               text  labels\n",
      "0  train_pid_7991  At this point just genuinely curious what sort...       0\n",
      "1  train_pid_7992  I have literally tried everything.... : I'm st...       0\n",
      "2  train_pid_7995  I'm really struggling : So I don't know how to...       0\n",
      "3  train_pid_7996  My meds aren’t making my depression any better...       0\n",
      "4  train_pid_7997  Hi I'm unwell : I'm 21 now, \"vice ridden\", and...       0\n",
      "Index(['pid', 'text', 'labels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your CSVs\n",
    "train_df = pd.read_csv(\"train (1).csv\")\n",
    "test_df = pd.read_csv(\"test (1).csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "\n",
    "# Peek data\n",
    "print(train_df.head())\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f1b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    3101\n",
       "2    2255\n",
       "0     650\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7a779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label 0 samples:\n",
      "\n",
      "1. Having a break down and wanted to hurt myself : For some reason today im having a anxiety panic attack and suddenly i just wanted to cry. I wanted to talk to someone so bad but im to scared that people think im annoying! I hide in my bathroom and cry nonstop for 2 dam hours straight and luckily i didn't bring any razor so i won't cut myself.... My relationship are toxic! It so toxic that i never think something good about myself and everyone around make me believe that. In reality i just need to stand up for myself - im a pathetic loser that to scare to committed to things, and i won't change it because im a coward ! Ha i sound so mess up and stupid dont i :)\n",
      "2. Not sure who else needs this, but you've made it through another day of existing. : I know how hard it is to be here sometimes, especially when the medicines that you're supposed to use aren't helping as much as you'd like them to, or the people around you don't seem to know how much you're struggling internally.\n",
      "If you've made it this far into your day, though, or even if it's the middle of the night, and you just hopped online to scroll through Reddit, I just want to congratulate you. No matter what happened today, good job getting through it. I'm proud of everyone here. :)\n",
      "3. I haven’t eaten a proper filling meal in weeks. : I’ve been on Zoloft for two years, I’m basically on meds so that I don’t have such intense lows and possibly off myself.\n",
      "I still deal with depression but it’s not nearly as bad as it was in 2018(my absolute lowest) sometimes I forget that even tho my emotional symptoms aren’t as present I could still be better.\n",
      "I’ve kinda forgot how it feels to be mentally ok and stable. I forget that I can still take better care of myself even if \"I’ve been worse\".\n",
      "I’m trying to become human again, making sure to shower several times a week, going outside and eating proper food. \n",
      "I’ve kinda just lived off instant noodles, grilled cheese, chips and snacks for a long time. I forget make proper meals for myself.\n",
      "Anyway, I’m currently making myself some proper food and hope you all remember to care for yourself too.\n",
      "Drink some water, eat a good meal and remember you can get better. You’re not actually stuck in a rut, it’s hard to get out of it, but it’s possible. Take one day at a time and do your best.\n",
      "\n",
      "Label 1 samples:\n",
      "\n",
      "1. Why the fuck did I make it past 2020? : I dont know if I just like to be constantly suffering or something. But I cannot comprehend why I'm still breathing. I guess I could say I pussied out and didn't kill myself already. But idk, probably. I've already been on this fucking planet for over a decade and a half, I've already seen theres no fucking point in any of us. We just killed everything we touch. And soon the whole fucking world is going to die.\n",
      "\n",
      "It doesnt fucking help that I'm contributing zilch to society. \"Oh, you're still a teenager, you're still developing.\" \n",
      "\n",
      "Yeah, you mean the 5 year long plateau in every single fucking thing I try. Or how I've literally started to slur and stutter with my sentences, meshing them into one blob of a sound. Or how about my drawing capabilities dipping in quality, now being about as good as a kindergarteners art project. If that isnt digression in development then idk what the fuck is.\n",
      "\n",
      "I'm literally becoming worse and worse by the day, my mental state is like a fucking volcano ready to explode, my body seems to be slowing down and feeling more and more groggy, I've been getting more and more sick, my bones are starting to ache, my skills in art is diminished, my speech is going to shit, my social skills have always been shitty and piss poor.\n",
      "\n",
      "And now my depression or whatever the fuck it is, is apart of me.\n",
      "\n",
      "It's who I am, a negative and vile piece of trash, I CANT EVEN FUCKING EMOTE ANYMORE. MY BODY FEELS LIKE AN EMPTY SHELL.\n",
      "\n",
      "why cant someone else just fucking kill me\n",
      "\n",
      "If my mind and my body keep on going like this, then I'm going to kill myself before I graduate\n",
      "\n",
      "Because the world doesnt need someone who cant even fucking achieve mediocre things.\n",
      "\n",
      "I should move aside for the actual fucking important people to help the world\n",
      "2. Sorry this might be triggering : I want to kill myself, I’m an alcoholic and yeah over fucking everyone around wanna end it cops took all my drugs so only got 10mg Valium and unlimited vodka and was just gonna get heaps of Panadol just wondering how many??? Don’t need anyone advzice to say nahhh I’m over it\n",
      "3. This is funny : My life. (I know, my jokes are lame af)\n",
      "But seriously tho, DISCLAIMER: I’m sorry if you clicked on this wanted to find something funny but there’s actually nothing here.\n",
      "The thing is, I’ve been unmotivated and ambitionless for white a longggg while now. And I don’t see the day that I become ambition at all. \n",
      "So because of that, I’ve been avoid dating a lottt. You know, it’s just... I don’t want to drag anyone down with me in the darkness. But I still feel very lonely and really needs someone since I’ve been alone for my entire life now. I haven’t had ANY dates since the day I was born and that makes me reallly craving for the feels that it will bring, maybe even giving me some motivation for god sake. But yeaaaaaa.... noooo.\n",
      "So even though I’m still not planning on dating anyone. I still want to feel a tini tiny bit better so I went on google and searched for “should ambtionless date” in hoping of some pages popping up saying “yes don’t feel bad” or something like “yes there are people who accepts you” . And guess what, they’re all no no no and no. Now I feel like a f*ckin useless piece of sheeet (not that I wasn’t feeling like that at the beginning, now I’m just feeling more useless) and just BAM. There goes all my esteem and hope for life haha.\n",
      "What am I trying to achieve with this post? I have no fkin clue. I guess I’m just out of my mind at 4:00 am and need to virtually SCREAM at something to feel maybe better. I fkin hate this. Thank you for reading. Maybe comment so I have some more things to scream at. Or maybe sorry for wasting your time, have a good day or night. But I guess after reading this sheet it will make you feel sheet too, well then I’m sorry but I will still post this just because. Sorry\n",
      "\n",
      "Label 2 samples:\n",
      "\n",
      "1. Does anyone else feel the urge to indulge in their loneliness and depression? : [removed]\n",
      "2. you're worth it : if you're browsing this sub, feeling bad about yourself, or if you're searching for people who understand you, or if you're just mindlessly scrolling, just stop for a moment. take this as a sign. you're okay. depression is so hard to battle. if you have the energy, do one of these things:\n",
      "1. go and get a glass of water\n",
      "2. walk around your house for a moment, stretch your legs\n",
      "3. message an old friend for a catch-up\n",
      "4. write all your feelings down in a journal\n",
      "you are worth recovery. you're worth the little things, like waking up to the sun and a fresh breeze, finding a joke that you genuinely laugh at, the possibility of meeting new people and getting better. because you *will* get better. this is all part of recovery, and you can do it.\n",
      "this is a sign to carry on, if you need one. you're loved, wanted, valued, needed.\n",
      "3. I cant live with my own failure : [removed]\n"
     ]
    }
   ],
   "source": [
    "# Show 3 random samples per label\n",
    "for label in train_df[\"labels\"].unique():\n",
    "    print(f\"\\nLabel {label} samples:\\n\")\n",
    "    samples = train_df[train_df[\"labels\"] == label].sample(3, random_state=42)[\"text\"]\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"{i}. {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061afb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pid', 'text', 'labels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af54dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "train_df['labels'] = encoder.fit_transform(train_df['labels'])\n",
    "test_df['labels'] = encoder.transform(test_df['labels'])\n",
    "dev_df['labels'] = encoder.transform(dev_df['labels'])\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "X_dev = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "y_train = train_df['labels'].values\n",
    "y_test = test_df['labels'].values\n",
    "y_dev = dev_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70dcd2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,420,035</span> (5.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,420,035\u001b[0m (5.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,420,035</span> (5.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,420,035\u001b[0m (5.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9456 - val_loss: 0.8481\n",
      "Epoch 2/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9209 - val_loss: 0.7961\n",
      "Epoch 3/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9201 - val_loss: 0.8109\n",
      "Epoch 4/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - loss: 0.9184 - val_loss: 0.8070\n",
      "Epoch 5/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9170 - val_loss: 0.7973\n",
      "Epoch 6/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9188 - val_loss: 0.8257\n",
      "Epoch 7/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9163 - val_loss: 0.8336\n",
      "Epoch 8/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9152 - val_loss: 0.8015\n",
      "Epoch 9/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - loss: 0.9167 - val_loss: 0.8225\n",
      "Epoch 10/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9143 - val_loss: 0.8034\n",
      "Epoch 11/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9127 - val_loss: 0.7841\n",
      "Epoch 12/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.9122 - val_loss: 0.8020\n",
      "Epoch 13/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9133 - val_loss: 0.8067\n",
      "Epoch 14/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.9138 - val_loss: 0.8087\n",
      "Epoch 15/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.9128 - val_loss: 0.7870\n",
      "Epoch 16/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9136 - val_loss: 0.8115\n",
      "Epoch 17/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9122 - val_loss: 0.7931\n",
      "Epoch 18/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9104 - val_loss: 0.8073\n",
      "Epoch 19/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9115 - val_loss: 0.8053\n",
      "Epoch 20/20\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 0.9131 - val_loss: 0.8229\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "max_len = 100\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Build the model explicitly\n",
    "model.build(input_shape=(None, max_len))\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',   # for integer-encoded labels\n",
    "    optimizer=optimizer,\n",
    ")  \n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_dev, y_dev),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bc64b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       228\n",
      "           1       0.69      0.83      0.76      2169\n",
      "           2       0.41      0.30      0.35       848\n",
      "\n",
      "    accuracy                           0.64      3245\n",
      "   macro avg       0.37      0.38      0.37      3245\n",
      "weighted avg       0.57      0.64      0.60      3245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = encoder.classes_.astype(str)  # ensures they are strings\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c21a965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6357473035439137\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       228\n",
      "           1       0.69      0.83      0.76      2169\n",
      "           2       0.41      0.30      0.35       848\n",
      "\n",
      "    accuracy                           0.64      3245\n",
      "   macro avg       0.37      0.38      0.37      3245\n",
      "weighted avg       0.57      0.64      0.60      3245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Pc\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert classes to string labels\n",
    "target_names = [str(label) for label in encoder.classes_]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classes))\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2f2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "Accuracy: 0.6357473035439137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classes))\n",
    "#print(classification_report(y_test, y_pred_classes, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b91a32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"train (1).csv\")\n",
    "\n",
    "# # Take one sample (first row)\n",
    "# sample_text = test_df['text'].iloc[5]  \n",
    "# print(\"Sample text:\", sample_text)\n",
    "\n",
    "sample_text=\"i am depressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebf6f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you already have the same tokenizer used in training\n",
    "maxlen=10\n",
    "seq = tokenizer.texts_to_sequences([sample_text])\n",
    "padded = pad_sequences(seq, maxlen=maxlen)  # same maxlen as training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d20cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(padded)\n",
    "\n",
    "# Convert to class\n",
    "pred_class = np.argmax(pred, axis=1)\n",
    "print(\"Predicted class:\", pred_class[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859d13d",
   "metadata": {},
   "source": [
    "# naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe5115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6776579352850539\n",
      "Dev Accuracy: 0.542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf = vectorizer.transform(test_df['text'])\n",
    "X_dev_tfidf = vectorizer.transform(dev_df['text'])\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_test = nb_model.predict(X_test_tfidf)\n",
    "y_pred_dev = nb_model.predict(X_dev_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Dev Accuracy:\", accuracy_score(y_dev, y_pred_dev))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f695480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Sample text to predict\n",
    "sample_text = [\"I feel hopeless and nothing seems to bring me joy anymore\"]  # replace with your text\n",
    "\n",
    "# Transform using the same TF-IDF vectorizer\n",
    "sample_tfidf = vectorizer.transform(sample_text)\n",
    "\n",
    "# Predict\n",
    "sample_pred = nb_model.predict(sample_tfidf)\n",
    "predicted_label = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "print(\"Predicted label:\", predicted_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7773aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open(\"nb_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(nb_model, model_file)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open(\"vectorizer.pkl\", \"wb\") as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "\n",
    "print(\"✅ Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5a034ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label encoder saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Example: fitting encoder\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_df['labels'])\n",
    "y_test = encoder.transform(test_df['labels'])\n",
    "y_dev = encoder.transform(dev_df['labels'])\n",
    "\n",
    "# --- Save encoder ---\n",
    "with open(\"label_encoder.pkl\", \"wb\") as enc_file:\n",
    "    pickle.dump(encoder, enc_file)\n",
    "\n",
    "print(\"✅ Label encoder saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
